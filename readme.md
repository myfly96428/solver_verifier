# Dual AI Chat - 双AI辩论聊天应用

一个创新的AI辩论平台，让两个AI角色（Cognito和Muse）进行结构化的深度辩论来解决复杂问题。

## 🆕 新功能：流式输出 + 移动窗口

现在支持实时流式输出和智能历史信息管理：

### 流式输出
- **实时观看AI思考过程**: 文字逐字显示，如同真实对话
- **流畅的用户体验**: 无需等待完整响应，立即看到AI开始生成内容
- **动态状态指示**: 清晰的视觉指示器显示当前流式状态
- **方案实时更新**: 核心方案和最终答案都支持流式显示

### 移动窗口机制
- **历史信息连续性**: Muse能够获取上一轮方案的完整辩论历史
- **智能信息过滤**: 根据不同角色需求提供相关的历史信息
- **避免重复批判**: 通过历史信息避免重复讨论已解决的问题
- **渐进式改进**: 基于历史轨迹进行更有针对性的批判和改进

## 功能特点

- **双AI辩论系统**: Cognito（逻辑分析型）vs Muse（批判质疑型）
- **🔥 实时流式输出**: 观看AI实时生成回应，提升交互体验
- **结构化辩论流程**: 多轮次、多周期的深度讨论
- **实时方案演进**: 观察解决方案在辩论中的逐步完善
- **灵活的模型配置**: 支持多种AI模型（GPT、Claude、Gemini等）
- **直观的用户界面**: 清晰展示辩论过程和方案演进

## 快速开始

### 环境要求
- Node.js 16+
- npm 或 yarn

### 安装步骤

1. 克隆项目
```bash
git clone <repository-url>
cd dual-ai-chat
```

2. 安装依赖
```bash
npm install
```

3. 配置环境变量（可选）
```bash
cp .env.example .env
# 编辑 .env 文件，设置默认的API配置
```

4. 启动应用
```bash
npm start
# 或者使用开发模式
npm run dev
```

5. 打开浏览器访问 `http://localhost:5173`

### Windows 用户
直接双击 `start.bat` 文件即可启动应用。

## 使用方法

1. **配置AI模型**: 在右侧配置面板中设置Cognito和Muse使用的AI模型
2. **设置API**: 输入您的API Key和Base URL
3. **提出问题**: 在底部输入框中输入您想要探讨的问题
4. **观察辩论**: 系统将自动进行多轮辩论，**实时流式显示**AI的思考过程
5. **查看结果**: 辩论结束后查看最终的综合解决方案

## 流式输出特性

### 视觉指示器
- **● 绿色脉动点**: 表示AI正在生成内容
- **流式背景**: 不同颜色背景表示不同的流式状态
- **实时更新**: 文字逐字显示，无延迟感

### 支持的流式场景
- ✅ 初始方案生成
- ✅ Muse批判回应（含历史信息）
- ✅ Cognito深化论述（专注当前挑战）
- ✅ 方案整合更新
- ✅ 最终答案生成

### 移动窗口信息提供
- **Muse首次批判**: 获取上一轮完整辩论历史
- **Cognito回应**: 专注当前挑战，不受历史干扰
- **Muse总结批判**: 获取上一轮辩论摘要用于对比分析

## 辩论流程

### 阶段说明
1. **初始化**: Cognito生成初步解决方案（流式显示）
2. **辩论循环** (最多8个周期):
   - Muse首次批判（流式显示）
   - Cognito回应与深化（流式显示）
   - Muse总结性批判（流式显示）
   - 方案整合更新（流式显示）
3. **最终答案**: 生成结构化的最终报告（流式显示）

### AI角色设定
- **Cognito** 🧠: 逻辑分析型AI，专注于构建严谨的解决方案
- **Muse** 🎭: 批判质疑型AI，从对立角度挑战和完善方案

## 支持的AI模型

- OpenAI GPT系列 (gpt-3.5-turbo, gpt-4, gpt-4-turbo)
- Anthropic Claude系列 (claude-3-sonnet, claude-3-opus)
- Google Gemini系列 (gemini-pro, gemini-1.5-pro, gemini-2.5-pro)
- 其他兼容OpenAI API格式的模型

## 技术架构

- **后端**: Node.js + Express + Socket.IO
- **前端**: 原生JavaScript + WebSocket
- **AI集成**: 支持多种AI API提供商，**支持流式响应**
- **实时通信**: WebSocket实现实时辩论展示和流式输出

## 配置说明

### API配置
- **API Key**: 您的AI服务提供商API密钥
- **Base URL**: API服务地址（默认支持OpenAI格式）

### 模型选择
- 建议为Cognito和Muse选择不同的模型以获得更丰富的辩论效果
- 可以使用相同模型但通过系统提示词实现角色差异化
- **流式输出**对所有支持的模型都有效

## 开发说明

### 项目结构
```
dual-ai-chat/
├── src/
│   ├── app.js              # 主服务器文件
│   └── debate-manager.js   # 辩论管理核心逻辑（支持流式输出）
├── public/
│   ├── index.html          # 前端页面
│   ├── script.js           # 前端JavaScript（流式处理）
│   └── style.css           # 样式文件（流式视觉效果）
├── package.json
├── start.bat              # Windows启动脚本
└── README.md
```

### 流式输出实现
- **后端**: 使用axios的stream响应类型处理SSE流
- **前端**: WebSocket实时接收流式数据并更新UI
- **视觉反馈**: CSS动画提供流畅的用户体验

### 扩展开发
- 修改 `debate-manager.js` 中的系统提示词来调整AI角色行为
- 在 `public/script.js` 中添加新的前端功能
- 通过 `src/app.js` 添加新的API端点

## 故障排除

### 常见问题
1. **连接失败**: 检查API Key和Base URL配置
2. **模型不支持**: 确认模型名称正确且API支持
3. **响应缓慢**: 某些模型响应时间较长，请耐心等待
4. **辩论中断**: 检查网络连接和API配额
5. **流式输出中断**: 检查网络稳定性和WebSocket连接

### 调试模式
启用开发模式获得更详细的日志：
```bash
npm run dev
```

## 许可证

MIT License

## 贡献

欢迎提交Issue和Pull Request来改进这个项目！特别欢迎对流式输出功能的优化建议。